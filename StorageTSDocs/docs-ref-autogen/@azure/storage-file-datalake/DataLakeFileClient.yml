### YamlMime:TSType
name: DataLakeFileClient
uid: '@azure/storage-file-datalake.DataLakeFileClient'
package: '@azure/storage-file-datalake'
summary: A DataLakeFileClient represents a URL to the Azure Storage file.
fullName: DataLakeFileClient
isPreview: false
isDeprecated: false
type: class
constructors:
  - name: 'DataLakeFileClient(string, Pipeline)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.constructor_1'
    package: '@azure/storage-file-datalake'
    summary: Creates an instance of DataLakeFileClient from url and pipeline.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'new DataLakeFileClient(url: string, pipeline: Pipeline)'
      parameters:
        - id: url
          type: string
          description: |-
            A Client string pointing to Azure Storage data lake file, such as
                                "https://myaccount.dfs.core.windows.net/filesystem/file".
                                You can append a SAS if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem/directory/file?sasString".
        - id: pipeline
          type: <xref uid="@azure/storage-file-datalake.Pipeline" />
          description: |-
            Call newPipeline() to create a default
                                       pipeline, or provide a customized pipeline.
  - name: >-
      DataLakeFileClient(string, StorageSharedKeyCredential |
      AnonymousCredential | TokenCredential, StoragePipelineOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.constructor'
    package: '@azure/storage-file-datalake'
    summary: Creates an instance of DataLakeFileClient from url and credential.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        new DataLakeFileClient(url: string, credential?:
        StorageSharedKeyCredential | AnonymousCredential | TokenCredential,
        options?: StoragePipelineOptions)
      parameters:
        - id: url
          type: string
          description: |-
            A Client string pointing to Azure Storage data lake file, such as
                                "https://myaccount.dfs.core.windows.net/filesystem/file".
                                You can append a SAS if using AnonymousCredential, such as "https://myaccount.dfs.core.windows.net/filesystem/directory/file?sasString".
        - id: credential
          type: >-
            <xref uid="@azure/storage-file-datalake.StorageSharedKeyCredential"
            /> | <xref uid="@azure/storage-file-datalake.AnonymousCredential" />
            | TokenCredential
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.StoragePipelineOptions" />
          description: ''
properties:
  - name: accountName
    uid: '@azure/storage-file-datalake.DataLakeFileClient.accountName'
    package: '@azure/storage-file-datalake'
    summary: ''
    fullName: accountName
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'accountName: string'
      return:
        type: string
        description: ''
  - name: credential
    uid: '@azure/storage-file-datalake.DataLakeFileClient.credential'
    package: '@azure/storage-file-datalake'
    summary: >-
      Such as AnonymousCredential, StorageSharedKeyCredential or any credential
      from the @azure/identity package to authenticate requests to the service.
      You can also provide an object that implements the TokenCredential
      interface. If not specified, AnonymousCredential is used.
    fullName: credential
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        credential: StorageSharedKeyCredential | AnonymousCredential |
        TokenCredential
      return:
        type: >-
          <xref uid="@azure/storage-file-datalake.StorageSharedKeyCredential" />
          | <xref uid="@azure/storage-file-datalake.AnonymousCredential" /> |
          TokenCredential
        description: ''
  - name: fileSystemName
    uid: '@azure/storage-file-datalake.DataLakeFileClient.fileSystemName'
    package: '@azure/storage-file-datalake'
    summary: Name of current file system.
    fullName: fileSystemName
    isPreview: false
    isDeprecated: false
    syntax:
      content: string fileSystemName
      return:
        type: string
        description: ''
  - name: name
    uid: '@azure/storage-file-datalake.DataLakeFileClient.name'
    package: '@azure/storage-file-datalake'
    summary: Name of current path (directory or file).
    fullName: name
    isPreview: false
    isDeprecated: false
    syntax:
      content: string name
      return:
        type: string
        description: ''
  - name: url
    uid: '@azure/storage-file-datalake.DataLakeFileClient.url'
    package: '@azure/storage-file-datalake'
    summary: Encoded URL string value.
    fullName: url
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'url: string'
      return:
        type: string
        description: ''
methods:
  - name: 'append(HttpRequestBody, number, number, FileAppendOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.append'
    package: '@azure/storage-file-datalake'
    summary: >-
      Uploads data to be appended to a file. Data can only be appended to a
      file.

      To apply perviously uploaded data to a file, call flush.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function append(body: HttpRequestBody, offset: number, length: number,
        options?: FileAppendOptions)
      parameters:
        - id: body
          type: HttpRequestBody
          description: Content to be uploaded.
        - id: offset
          type: number
          description: Append offset in bytes.
        - id: length
          type: number
          description: Length of content to append in bytes.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileAppendOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileAppendResponse"
          />&gt;
        description: ''
  - name: create(FileCreateOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.create_1'
    package: '@azure/storage-file-datalake'
    summary: Create a file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function create(options?: FileCreateOptions)'
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileCreateOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileCreateResponse"
          />&gt;
        description: ''
  - name: 'create(PathResourceType, PathCreateOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.create'
    package: '@azure/storage-file-datalake'
    summary: Create a file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function create(resourceType: PathResourceType, options?:
        PathCreateOptions)
      parameters:
        - id: resourceType
          type: <xref uid="@azure/storage-file-datalake.PathResourceType" />
          description: 'Resource type, must be "file" for DataLakeFileClient.'
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathCreateOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathCreateResponse"
          />&gt;
        description: ''
  - name: createIfNotExists(FileCreateIfNotExistsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.createIfNotExists_1'
    package: '@azure/storage-file-datalake'
    summary: Create a file if it doesn't already exists.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function createIfNotExists(options?: FileCreateIfNotExistsOptions)'
      parameters:
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.FileCreateIfNotExistsOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.FileCreateIfNotExistsResponse"
          />&gt;
        description: ''
  - name: 'createIfNotExists(PathResourceType, PathCreateIfNotExistsOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.createIfNotExists'
    package: '@azure/storage-file-datalake'
    summary: Create a file if it doesn't already exists.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function createIfNotExists(resourceType: PathResourceType, options?:
        PathCreateIfNotExistsOptions)
      parameters:
        - id: resourceType
          type: <xref uid="@azure/storage-file-datalake.PathResourceType" />
          description: 'Resource type, must be "file" for DataLakeFileClient.'
        - id: options
          type: >-
            <xref
            uid="@azure/storage-file-datalake.PathCreateIfNotExistsOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathCreateIfNotExistsResponse"
          />&gt;
        description: ''
  - name: 'delete(boolean, PathDeleteOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.delete'
    package: '@azure/storage-file-datalake'
    summary: Delete current path (directory or file).
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function delete(recursive?: boolean, options?: PathDeleteOptions)'
      parameters:
        - id: recursive
          type: boolean
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathDeleteOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathDeleteResponse"
          />&gt;
        description: ''
  - name: 'deleteIfExists(boolean, PathDeleteOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.deleteIfExists'
    package: '@azure/storage-file-datalake'
    summary: Delete current path (directory or file) if it exists.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function deleteIfExists(recursive?: boolean, options?:
        PathDeleteOptions)
      parameters:
        - id: recursive
          type: boolean
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathDeleteOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathDeleteIfExistsResponse" />&gt;
        description: ''
  - name: exists(PathExistsOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.exists'
    package: '@azure/storage-file-datalake'
    summary: >
      Returns true if the Data Lake file represented by this client exists;
      false otherwise.

      NOTE: use this function with care since an existing file might be deleted
      by other clients or

      applications. Vice versa new files might be added by other clients or
      applications after this

      function completes.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function exists(options?: PathExistsOptions)'
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathExistsOptions" />
          description: ''
      return:
        type: Promise&lt;boolean&gt;
        description: ''
  - name: 'flush(number, FileFlushOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.flush'
    package: '@azure/storage-file-datalake'
    summary: Flushes (writes) previously appended data to a file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function flush(position: number, options?: FileFlushOptions)'
      parameters:
        - id: position
          type: number
          description: |-
            File position to flush.
                                     This parameter allows the caller to upload data in parallel and control the order in which it is appended to the file.
                                     It is required when uploading data to be appended to the file and when flushing previously uploaded data to the file.
                                     The value must be the position where the data is to be appended. Uploaded data is not immediately flushed, or written,
                                     to the file. To flush, the previously uploaded data must be contiguous, the position parameter must be specified and
                                     equal to the length of the file after all data has been written, and there must not be a request entity body included
                                     with the request.
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileFlushOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathFlushDataResponse" />&gt;
        description: ''
  - name: getAccessControl(PathGetAccessControlOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getAccessControl'
    package: '@azure/storage-file-datalake'
    summary: Returns the access control data for a path (directory of file).
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function getAccessControl(options?: PathGetAccessControlOptions)'
      parameters:
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathGetAccessControlOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathGetAccessControlResponse" />&gt;
        description: ''
  - name: getDataLakeLeaseClient(string)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getDataLakeLeaseClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Get a <xref:DataLakeLeaseClient> that manages leases on the path
      (directory or file).
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function getDataLakeLeaseClient(proposeLeaseId?: string)'
      parameters:
        - id: proposeLeaseId
          type: string
          description: ''
      return:
        type: <xref uid="@azure/storage-file-datalake.DataLakeLeaseClient" />
        description: ''
  - name: getProperties(PathGetPropertiesOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.getProperties'
    package: '@azure/storage-file-datalake'
    summary: >
      Returns all user-defined metadata, standard HTTP properties, and system
      properties

      for the path (directory or file).

      WARNING: The `metadata` object returned in the response will have its keys
      in lowercase, even if

      they originally contained uppercase characters. This differs from the
      metadata keys returned by

      the methods of <xref:DataLakeFileSystemClient> that list paths using the
      `includeMetadata` option, which

      will retain their original casing.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function getProperties(options?: PathGetPropertiesOptions)'
      parameters:
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathGetPropertiesOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathGetPropertiesResponse" />&gt;
        description: ''
  - name: 'move(string, PathMoveOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.move'
    package: '@azure/storage-file-datalake'
    summary: Move directory or file within same file system.
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function move(destinationPath: string, options?: PathMoveOptions)'
      parameters:
        - id: destinationPath
          type: string
          description: >-
            Destination directory path like "directory" or file path
            "directory/file"
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathMoveOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathMoveResponse"
          />&gt;
        description: ''
  - name: 'move(string, string, PathMoveOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.move_1'
    package: '@azure/storage-file-datalake'
    summary: Move directory or file to another file system.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function move(destinationFileSystem: string, destinationPath: string,
        options?: PathMoveOptions)
      parameters:
        - id: destinationFileSystem
          type: string
          description: Destination file system like "filesystem".
        - id: destinationPath
          type: string
          description: >-
            Destination directory path like "directory" or file path
            "directory/file"
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathMoveOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.PathMoveResponse"
          />&gt;
        description: ''
  - name: 'query(string, FileQueryOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.query'
    package: '@azure/storage-file-datalake'
    summary: >
      Quick query for a JSON or CSV formatted file.

      Example usage (Node.js):


      ```js

      // Query and convert a file to a string

      const queryResponse = await fileClient.query("select * from BlobStorage");

      const downloaded = (await
      streamToBuffer(queryResponse.readableStreamBody)).toString();

      console.log("Query file content:", downloaded);


      async function streamToBuffer(readableStream) {
        return new Promise((resolve, reject) => {
          const chunks = [];
          readableStream.on("data", (data) => {
            chunks.push(data instanceof Buffer ? data : Buffer.from(data));
          });
          readableStream.on("end", () => {
            resolve(Buffer.concat(chunks));
          });
          readableStream.on("error", reject);
        });
      }

      ```
    isPreview: false
    isDeprecated: false
    syntax:
      content: 'function query(query: string, options?: FileQueryOptions)'
      parameters:
        - id: query
          type: string
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileQueryOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
        description: ''
  - name: 'read(number, number, FileReadOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.read'
    package: '@azure/storage-file-datalake'
    summary: |
      Downloads a file from the service, including its metadata and properties.
      * In Node.js, data returns in a Readable stream readableStreamBody
      * In browsers, data returns in a promise contentAsBlob
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function read(offset?: number, count?: number, options?:
        FileReadOptions)
      parameters:
        - id: offset
          type: number
          description: ''
        - id: count
          type: number
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
        description: ''
  - name: 'readToBuffer(Buffer, number, number, FileReadToBufferOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToBuffer'
    package: '@azure/storage-file-datalake'
    summary: >
      ONLY AVAILABLE IN NODE.JS RUNTIME.

      Reads a Data Lake file in parallel to a buffer.

      Offset and count are optional, pass 0 for both to read the entire file.


      Warning: Buffers can only support files up to about one gigabyte on 32-bit
      systems or about two

      gigabytes on 64-bit systems due to limitations of Node.js/V8. For files
      larger than this size,

      consider <xref:readToFile>.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function readToBuffer(buffer: Buffer, offset?: number, count?: number,
        options?: FileReadToBufferOptions)
      parameters:
        - id: buffer
          type: Buffer
          description: 'Buffer to be fill, must have length larger than count'
        - id: offset
          type: number
          description: From which position of the Data Lake file to read
        - id: count
          type: number
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadToBufferOptions" />
          description: ''
      return:
        type: Promise&lt;Buffer&gt;
        description: ''
  - name: 'readToBuffer(number, number, FileReadToBufferOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToBuffer_1'
    package: '@azure/storage-file-datalake'
    summary: >
      ONLY AVAILABLE IN NODE.JS RUNTIME

      Reads a Data Lake file in parallel to a buffer.

      Offset and count are optional, pass 0 for both to read the entire file


      Warning: Buffers can only support files up to about one gigabyte on 32-bit
      systems or about two

      gigabytes on 64-bit systems due to limitations of Node.js/V8. For files
      larger than this size,

      consider <xref:readToFile>.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function readToBuffer(offset?: number, count?: number, options?:
        FileReadToBufferOptions)
      parameters:
        - id: offset
          type: number
          description: From which position of the Data Lake file to read(in bytes)
        - id: count
          type: number
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadToBufferOptions" />
          description: ''
      return:
        type: Promise&lt;Buffer&gt;
        description: ''
  - name: 'readToFile(string, number, number, FileReadOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.readToFile'
    package: '@azure/storage-file-datalake'
    summary: >
      ONLY AVAILABLE IN NODE.JS RUNTIME.

      Downloads a Data Lake file to a local file.

      Fails if the the given file path already exits.

      Offset and count are optional, pass 0 and undefined respectively to
      download the entire file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function readToFile(filePath: string, offset?: number, count?: number,
        options?: FileReadOptions)
      parameters:
        - id: filePath
          type: string
          description: ''
        - id: offset
          type: number
          description: ''
        - id: count
          type: number
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.FileReadOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref uid="@azure/storage-file-datalake.FileReadResponse"
          />&gt;
        description: |-
          The response data for file read operation,
                                               but with readableStreamBody set to undefined since its
                                               content is already read and written into a local file
                                               at the specified path.
  - name: 'setAccessControl(PathAccessControlItem[], PathSetAccessControlOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setAccessControl'
    package: '@azure/storage-file-datalake'
    summary: Set the access control data for a path (directory of file).
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function setAccessControl(acl: PathAccessControlItem[], options?:
        PathSetAccessControlOptions)
      parameters:
        - id: acl
          type: '<xref uid="@azure/storage-file-datalake.PathAccessControlItem" />[]'
          description: The POSIX access control list for the file or directory.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetAccessControlOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetAccessControlResponse" />&gt;
        description: ''
  - name: 'setHttpHeaders(PathHttpHeaders, PathSetHttpHeadersOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setHttpHeaders'
    package: '@azure/storage-file-datalake'
    summary: >-
      Sets system properties on the path (directory or file).

      If no value provided, or no value provided for the specified blob HTTP
      headers,

      these blob HTTP headers without a value will be cleared.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function setHttpHeaders(httpHeaders: PathHttpHeaders, options?:
        PathSetHttpHeadersOptions)
      parameters:
        - id: httpHeaders
          type: <xref uid="@azure/storage-file-datalake.PathHttpHeaders" />
          description: ''
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetHttpHeadersOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetHttpHeadersResponse" />&gt;
        description: ''
  - name: 'setMetadata(Metadata, PathSetMetadataOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setMetadata'
    package: '@azure/storage-file-datalake'
    summary: >
      Sets user-defined metadata for the specified path (directory of file) as
      one or more name-value pairs.

      If no option provided, or no metadata defined in the parameter, the path

      metadata will be removed.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function setMetadata(metadata?: Metadata, options?:
        PathSetMetadataOptions)
      parameters:
        - id: metadata
          type: <xref uid="@azure/storage-file-datalake.Metadata" />
          description: ''
        - id: options
          type: <xref uid="@azure/storage-file-datalake.PathSetMetadataOptions" />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetMetadataResponse" />&gt;
        description: ''
  - name: 'setPermissions(PathPermissions, PathSetPermissionsOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.setPermissions'
    package: '@azure/storage-file-datalake'
    summary: Sets the file permissions on a path.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function setPermissions(permissions: PathPermissions, options?:
        PathSetPermissionsOptions)
      parameters:
        - id: permissions
          type: <xref uid="@azure/storage-file-datalake.PathPermissions" />
          description: >-
            The POSIX access permissions for the file owner, the file owning
            group, and others.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.PathSetPermissionsOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathSetAccessControlResponse" />&gt;
        description: ''
  - name: toDirectoryClient()
    uid: '@azure/storage-file-datalake.DataLakeFileClient.toDirectoryClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Convert current DataLakePathClient to DataLakeDirectoryClient if current
      path is a directory.
    isPreview: false
    isDeprecated: false
    syntax:
      content: function toDirectoryClient()
      return:
        type: <xref uid="@azure/storage-file-datalake.DataLakeDirectoryClient" />
        description: ''
  - name: toFileClient()
    uid: '@azure/storage-file-datalake.DataLakeFileClient.toFileClient'
    package: '@azure/storage-file-datalake'
    summary: >-
      Convert current DataLakePathClient to DataLakeFileClient if current path
      is a file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: function toFileClient()
      return:
        type: <xref uid="@azure/storage-file-datalake.DataLakeFileClient" />
        description: ''
  - name: >-
      upload(Buffer | Blob | ArrayBuffer | ArrayBufferView,
      FileParallelUploadOptions)
    uid: '@azure/storage-file-datalake.DataLakeFileClient.upload'
    package: '@azure/storage-file-datalake'
    summary: Uploads a Buffer(Node.js)/Blob/ArrayBuffer/ArrayBufferView to a File.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function upload(data: Buffer | Blob | ArrayBuffer | ArrayBufferView,
        options?: FileParallelUploadOptions)
      parameters:
        - id: data
          type: Buffer | Blob | ArrayBuffer | ArrayBufferView
          description: 'Buffer(Node), Blob, ArrayBuffer or ArrayBufferView'
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathFlushDataResponse" />&gt;
        description: ''
  - name: 'uploadFile(string, FileParallelUploadOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.uploadFile'
    package: '@azure/storage-file-datalake'
    summary: |
      ONLY AVAILABLE IN NODE.JS RUNTIME.
      Uploads a local file to a Data Lake file.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function uploadFile(filePath: string, options?:
        FileParallelUploadOptions)
      parameters:
        - id: filePath
          type: string
          description: Full path of the local file
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathFlushDataResponse" />&gt;
        description: ''
  - name: 'uploadStream(Readable, FileParallelUploadOptions)'
    uid: '@azure/storage-file-datalake.DataLakeFileClient.uploadStream'
    package: '@azure/storage-file-datalake'
    summary: >
      ONLY AVAILABLE IN NODE.JS RUNTIME.

      Uploads a Node.js Readable stream into a Data Lake file.

      This method will try to create a file, then starts uploading chunk by
      chunk.

      Please make sure potential size of stream doesn't exceed
      FILE_MAX_SIZE_BYTES and

      potential number of chunks doesn't exceed BLOCK_BLOB_MAX_BLOCKS.


      PERFORMANCE IMPROVEMENT TIPS:

      * Input stream highWaterMark is better to set a same value with
      options.chunkSize
        parameter, which will avoid Buffer.concat() operations.
    isPreview: false
    isDeprecated: false
    syntax:
      content: >-
        function uploadStream(stream: Readable, options?:
        FileParallelUploadOptions)
      parameters:
        - id: stream
          type: Readable
          description: Node.js Readable stream.
        - id: options
          type: >-
            <xref uid="@azure/storage-file-datalake.FileParallelUploadOptions"
            />
          description: ''
      return:
        type: >-
          Promise&lt;<xref
          uid="@azure/storage-file-datalake.PathFlushDataResponse" />&gt;
        description: ''
extends: <xref uid="@azure/storage-file-datalake.DataLakePathClient" />
